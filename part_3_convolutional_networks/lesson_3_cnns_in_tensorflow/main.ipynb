{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quiz: Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    F_W = tf.Variable(tf.truncated_normal([2, 2, 1, 3]))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'SAME'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "#     return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "    return tf.nn.bias_add(tf.nn.conv2d(input, F_W, strides, padding), F_b)\n",
    "\n",
    "out = conv2d(X)\n",
    "\n",
    "b_tmp = tf.constant([1.0, 2.0, 3.0])\n",
    "\n",
    "# Lecture Note: The tf.nn.bias_add() function adds a 1-d bias to the last dimension in a matrix. (Note: using tf.add() doesn't work when the tensors aren't the same shape.)\n",
    "# Rui test: all three works here\n",
    "# out_tmp = out + b_tmp\n",
    "# out_tmp = tf.add(out, b_tmp)\n",
    "out_tmp = tf.nn.bias_add(out, b_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 3)\n",
      "[[[[ -3.61456823  -5.82835484  -0.91199523]\n",
      "   [  6.4423337   14.23735046   4.33617592]]\n",
      "\n",
      "  [[  1.00528979 -19.34800911 -17.04005432]\n",
      "   [ -1.41689062  -4.32622719  -4.27193022]]]]\n",
      "[[-3.61456823  6.4423337 ]\n",
      " [ 1.00528979 -1.41689062]]\n",
      "[[ -5.82835484  14.23735046]\n",
      " [-19.34800911  -4.32622719]]\n",
      "[[ -0.91199523   4.33617592]\n",
      " [-17.04005432  -4.27193022]]\n",
      "b_tmp_val\n",
      " [ 1.  2.  3.]\n",
      "out_tmp_val\n",
      " [[[[ -2.61456823  -3.82835484   2.08800483]\n",
      "   [  7.4423337   16.23735046   7.33617592]]\n",
      "\n",
      "  [[  2.00528979 -17.34800911 -14.04005432]\n",
      "   [ -0.41689062  -2.32622719  -1.27193022]]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out_val, b_tmp_val, out_tmp_val = sess.run([out, b_tmp, out_tmp])\n",
    "    print(out_val.shape)\n",
    "    print(out_val)\n",
    "    for i in range(3):\n",
    "        print(out_val[0, :, :, i]) \n",
    "        \n",
    "    print('b_tmp_val\\n', b_tmp_val)\n",
    "    print('out_tmp_val\\n', out_tmp_val)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    F_W = tf.Variable(tf.truncated_normal([3, 3, 1, 3]))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 1, 1, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'VALID'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "\n",
    "out = conv2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 3)\n",
      "[[[[ -8.94327641   9.03826332  -9.48495197]\n",
      "   [-15.12922287   1.65697861  -6.69941425]]\n",
      "\n",
      "  [[-13.37130928  29.46146202 -25.25383949]\n",
      "   [ -8.12068176   6.2870245   -1.013906  ]]]]\n",
      "[[ -8.94327641 -15.12922287]\n",
      " [-13.37130928  -8.12068176]]\n",
      "[[  9.03826332   1.65697861]\n",
      " [ 29.46146202   6.2870245 ]]\n",
      "[[ -9.48495197  -6.69941425]\n",
      " [-25.25383949  -1.013906  ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out_val = sess.run(out)\n",
    "    print(out_val.shape)\n",
    "    print(out_val)\n",
    "    for i in range(3):\n",
    "        print(out_val[0, :, :, i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quiz: Max Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the values to `strides` and `ksize` such that\n",
    "the output shape after pooling is (1, 2, 2, 1).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "def maxpool(input):\n",
    "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'SAME'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "    \n",
    "out = maxpool(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out_val= sess.run(out)\n",
    "    print(out_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CNNs in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# number of samples to calculate validation and accuracy\n",
    "# decrease this if you're running out of memory\n",
    "test_valid_size = 256\n",
    "\n",
    "# network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # dropout (probability to keep units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store weights & biases\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 56891.6328 Validation Accuracy: 0.070312\n",
      "Epoch  1, Batch   2 -Loss: 46594.3984 Validation Accuracy: 0.078125\n",
      "Epoch  1, Batch   3 -Loss: 37274.9844 Validation Accuracy: 0.089844\n",
      "Epoch  1, Batch   4 -Loss: 34622.7891 Validation Accuracy: 0.089844\n",
      "Epoch  1, Batch   5 -Loss: 30409.2266 Validation Accuracy: 0.089844\n",
      "Epoch  1, Batch   6 -Loss: 27175.3965 Validation Accuracy: 0.109375\n",
      "Epoch  1, Batch   7 -Loss: 23384.9805 Validation Accuracy: 0.113281\n",
      "Epoch  1, Batch   8 -Loss: 25059.6426 Validation Accuracy: 0.128906\n",
      "Epoch  1, Batch   9 -Loss: 24200.0312 Validation Accuracy: 0.152344\n",
      "Epoch  1, Batch  10 -Loss: 20925.2480 Validation Accuracy: 0.144531\n",
      "Epoch  1, Batch  11 -Loss: 20335.0078 Validation Accuracy: 0.167969\n",
      "Epoch  1, Batch  12 -Loss: 22964.1738 Validation Accuracy: 0.171875\n",
      "Epoch  1, Batch  13 -Loss: 20590.6973 Validation Accuracy: 0.167969\n",
      "Epoch  1, Batch  14 -Loss: 18382.9648 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  15 -Loss: 17838.3398 Validation Accuracy: 0.199219\n",
      "Epoch  1, Batch  16 -Loss: 17171.1465 Validation Accuracy: 0.183594\n",
      "Epoch  1, Batch  17 -Loss: 15341.8125 Validation Accuracy: 0.199219\n",
      "Epoch  1, Batch  18 -Loss: 21184.0586 Validation Accuracy: 0.210938\n",
      "Epoch  1, Batch  19 -Loss: 16282.0293 Validation Accuracy: 0.238281\n",
      "Epoch  1, Batch  20 -Loss: 15831.2119 Validation Accuracy: 0.250000\n",
      "Epoch  1, Batch  21 -Loss: 15065.6680 Validation Accuracy: 0.246094\n",
      "Epoch  1, Batch  22 -Loss: 15966.9160 Validation Accuracy: 0.246094\n",
      "Epoch  1, Batch  23 -Loss: 13956.1074 Validation Accuracy: 0.250000\n",
      "Epoch  1, Batch  24 -Loss: 15091.9199 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  25 -Loss: 15284.5078 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  26 -Loss: 14255.6670 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  27 -Loss: 14265.2666 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  28 -Loss: 12914.5615 Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch  29 -Loss: 13298.6738 Validation Accuracy: 0.285156\n",
      "Epoch  1, Batch  30 -Loss: 11794.1699 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  31 -Loss: 12292.0400 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  32 -Loss: 11410.2207 Validation Accuracy: 0.312500\n",
      "Epoch  1, Batch  33 -Loss: 11926.4648 Validation Accuracy: 0.312500\n",
      "Epoch  1, Batch  34 -Loss: 10229.7598 Validation Accuracy: 0.324219\n",
      "Epoch  1, Batch  35 -Loss: 11108.0918 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  36 -Loss: 11297.2617 Validation Accuracy: 0.312500\n",
      "Epoch  1, Batch  37 -Loss: 10873.9219 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  38 -Loss: 11777.4824 Validation Accuracy: 0.324219\n",
      "Epoch  1, Batch  39 -Loss:  9688.5430 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  40 -Loss:  7936.4844 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  41 -Loss:  9327.0312 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  42 -Loss:  9330.3301 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  43 -Loss: 10307.3945 Validation Accuracy: 0.390625\n",
      "Epoch  1, Batch  44 -Loss:  9276.4941 Validation Accuracy: 0.378906\n",
      "Epoch  1, Batch  45 -Loss:  7420.7104 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  46 -Loss: 10061.2236 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  47 -Loss:  8238.3311 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  48 -Loss:  9019.0938 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  49 -Loss: 10176.1426 Validation Accuracy: 0.425781\n",
      "Epoch  1, Batch  50 -Loss:  8796.3604 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  51 -Loss:  8471.4141 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  52 -Loss:  8579.0801 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  53 -Loss:  6984.3013 Validation Accuracy: 0.406250\n",
      "Epoch  1, Batch  54 -Loss:  7729.2148 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  55 -Loss:  7447.3105 Validation Accuracy: 0.429688\n",
      "Epoch  1, Batch  56 -Loss:  9889.6543 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  57 -Loss:  7514.4897 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  58 -Loss:  7784.1865 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  59 -Loss:  6565.5967 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  60 -Loss:  6971.9473 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  61 -Loss:  6607.4316 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  62 -Loss:  8268.7051 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  63 -Loss:  6670.9937 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch  64 -Loss:  7867.7139 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  65 -Loss:  6335.4971 Validation Accuracy: 0.468750\n",
      "Epoch  1, Batch  66 -Loss:  6008.4048 Validation Accuracy: 0.472656\n",
      "Epoch  1, Batch  67 -Loss:  6743.2842 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  68 -Loss:  7378.5381 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch  69 -Loss:  5705.9507 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  70 -Loss:  5583.4385 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  71 -Loss:  6572.0068 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  72 -Loss:  6799.2954 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  73 -Loss:  6732.9390 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  74 -Loss:  4955.3901 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  75 -Loss:  6025.3809 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch  76 -Loss:  5895.8140 Validation Accuracy: 0.496094\n",
      "Epoch  1, Batch  77 -Loss:  5693.8408 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  78 -Loss:  4868.9004 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch  79 -Loss:  6705.3501 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  80 -Loss:  6034.9131 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  81 -Loss:  4777.8442 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  82 -Loss:  4703.8350 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  83 -Loss:  7039.4580 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  84 -Loss:  5602.6094 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  85 -Loss:  4477.3110 Validation Accuracy: 0.546875\n",
      "Epoch  1, Batch  86 -Loss:  4788.4111 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  87 -Loss:  5482.0303 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  88 -Loss:  5112.3750 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  89 -Loss:  4412.9004 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  90 -Loss:  4994.5801 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  91 -Loss:  5033.5381 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  92 -Loss:  4142.0996 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  93 -Loss:  5554.2480 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch  94 -Loss:  6248.1318 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch  95 -Loss:  4559.0918 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  96 -Loss:  4231.3149 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  97 -Loss:  4354.4136 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  98 -Loss:  5892.1709 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch  99 -Loss:  5153.2607 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch 100 -Loss:  4079.4197 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 101 -Loss:  3112.5029 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 102 -Loss:  5537.8921 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 103 -Loss:  4226.6299 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 104 -Loss:  4312.4507 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch 105 -Loss:  3851.5364 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 106 -Loss:  4984.6309 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 107 -Loss:  4060.1550 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 108 -Loss:  4419.7788 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 109 -Loss:  4079.0264 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 110 -Loss:  3773.5571 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 111 -Loss:  3736.1670 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 112 -Loss:  4094.6648 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 113 -Loss:  4349.5610 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 114 -Loss:  3739.3660 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 115 -Loss:  4405.5908 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 116 -Loss:  4753.5654 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 117 -Loss:  3858.6091 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 118 -Loss:  3726.5200 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch 119 -Loss:  3388.0317 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 120 -Loss:  4067.8230 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 121 -Loss:  3150.5945 Validation Accuracy: 0.582031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 122 -Loss:  3817.3621 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 123 -Loss:  3842.1741 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 124 -Loss:  3129.9062 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 125 -Loss:  3934.8916 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 126 -Loss:  3225.8979 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 127 -Loss:  3820.1223 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 128 -Loss:  3191.0347 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 129 -Loss:  4429.4180 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 130 -Loss:  3583.1167 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 131 -Loss:  3718.4319 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 132 -Loss:  3728.9287 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 133 -Loss:  3733.8628 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 134 -Loss:  3523.2690 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 135 -Loss:  2017.6597 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 136 -Loss:  3874.8965 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 137 -Loss:  3548.3889 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 138 -Loss:  3221.4316 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 139 -Loss:  3392.2578 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 140 -Loss:  3979.5046 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 141 -Loss:  3439.9268 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 142 -Loss:  3077.8267 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 143 -Loss:  4045.4131 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 144 -Loss:  2749.6799 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 145 -Loss:  4670.5845 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 146 -Loss:  3342.8047 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 147 -Loss:  3576.6316 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 148 -Loss:  3682.5049 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 149 -Loss:  3091.2993 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 150 -Loss:  4642.6221 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 151 -Loss:  3405.8750 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 152 -Loss:  2657.8752 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 153 -Loss:  3841.3032 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 154 -Loss:  3125.9163 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 155 -Loss:  2736.5327 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 156 -Loss:  3162.5581 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 157 -Loss:  3535.1311 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 158 -Loss:  3332.5630 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 159 -Loss:  3844.3572 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 160 -Loss:  2805.7019 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 161 -Loss:  2643.1362 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 162 -Loss:  2356.0146 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 163 -Loss:  2534.7114 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 164 -Loss:  2686.0061 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 165 -Loss:  2922.4072 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 166 -Loss:  2877.4949 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 167 -Loss:  2792.7371 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 168 -Loss:  2958.2920 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 169 -Loss:  2744.6841 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 170 -Loss:  2604.2356 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 171 -Loss:  3425.6240 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 172 -Loss:  2559.7754 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 173 -Loss:  2492.4941 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 174 -Loss:  3217.8018 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 175 -Loss:  2529.9326 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 176 -Loss:  2744.6567 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 177 -Loss:  3891.2573 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 178 -Loss:  3171.7903 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 179 -Loss:  2740.3198 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 180 -Loss:  3625.7607 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 181 -Loss:  2519.9468 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 182 -Loss:  2921.9189 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 183 -Loss:  2759.1523 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 184 -Loss:  3374.2014 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 185 -Loss:  2876.9902 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 186 -Loss:  2180.8728 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 187 -Loss:  2305.8665 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 188 -Loss:  2432.1819 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 189 -Loss:  2223.7744 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 190 -Loss:  3387.7151 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 191 -Loss:  2494.2097 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 192 -Loss:  3090.4412 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 193 -Loss:  2780.8611 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 194 -Loss:  3038.8042 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 195 -Loss:  2044.8286 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 196 -Loss:  2839.6599 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 197 -Loss:  2867.1357 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 198 -Loss:  2633.1035 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 199 -Loss:  2994.5454 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 200 -Loss:  2103.1423 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 201 -Loss:  2294.0964 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 202 -Loss:  2789.0811 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 203 -Loss:  2716.5547 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 204 -Loss:  2821.0649 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 205 -Loss:  2600.4138 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 206 -Loss:  1693.9070 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 207 -Loss:  2783.8579 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 208 -Loss:  2512.7686 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 209 -Loss:  2762.5247 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 210 -Loss:  2827.9272 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 211 -Loss:  2444.9819 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 212 -Loss:  2306.2905 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 213 -Loss:  2732.3132 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 214 -Loss:  2511.0303 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 215 -Loss:  2072.1995 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 216 -Loss:  2785.1641 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 217 -Loss:  2602.4773 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 218 -Loss:  1683.0352 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 219 -Loss:  2362.5654 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 220 -Loss:  2087.1455 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 221 -Loss:  2852.2363 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 222 -Loss:  2339.0181 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 223 -Loss:  1974.0051 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 224 -Loss:  2328.5327 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 225 -Loss:  1685.7112 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 226 -Loss:  2524.7930 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 227 -Loss:  2434.9937 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 228 -Loss:  2295.1594 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 229 -Loss:  2430.4321 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 230 -Loss:  2874.2598 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 231 -Loss:  2209.6287 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 232 -Loss:  1482.1113 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 233 -Loss:  2646.1331 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 234 -Loss:  1968.3103 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 235 -Loss:  2452.4648 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 236 -Loss:  2138.4834 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 237 -Loss:  1450.7601 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 238 -Loss:  2518.1523 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 239 -Loss:  1928.9941 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 240 -Loss:  1897.7865 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 241 -Loss:  2313.8135 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 242 -Loss:  1993.3206 Validation Accuracy: 0.699219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 243 -Loss:  2221.7991 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 244 -Loss:  1799.3040 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 245 -Loss:  2014.8044 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 246 -Loss:  2317.6743 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 247 -Loss:  2491.1558 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 248 -Loss:  1423.5707 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 249 -Loss:  2221.0195 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 250 -Loss:  2872.6406 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 251 -Loss:  1630.3405 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 252 -Loss:  1577.6605 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 253 -Loss:  1816.7122 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 254 -Loss:  1828.1387 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 255 -Loss:  1517.9529 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 256 -Loss:  2303.6494 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 257 -Loss:  2116.2275 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 258 -Loss:  1172.6823 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 259 -Loss:  1561.4143 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 260 -Loss:  1823.3484 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 261 -Loss:  2627.7732 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 262 -Loss:  2261.7988 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 263 -Loss:  1830.0555 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 264 -Loss:  1567.0344 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 265 -Loss:  2498.8574 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 266 -Loss:  2510.9431 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 267 -Loss:  2419.7839 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 268 -Loss:  2010.8342 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 269 -Loss:  2171.5574 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 270 -Loss:  1931.2534 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 271 -Loss:  2175.7634 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 272 -Loss:  1813.9202 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 273 -Loss:  1697.4509 Validation Accuracy: 0.691406\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
