{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Perceptrons as Logical Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -1.5                    0          Yes\n",
      "      0          1                  -0.5                    0          Yes\n",
      "      1          0                  -0.5                    0          Yes\n",
      "      1          1                   0.5                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -1.5\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Perceptron\n",
    "    \n",
    "Two options:    \n",
    "\n",
    "    Increases weights\n",
    "    Decrease magnitude of bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   0.5                    1          Yes\n",
      "      0          1                  -0.5                    0          Yes\n",
      "      1          0                   0.5                    1          Yes\n",
      "      1          1                  -0.5                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.0\n",
    "bias = 0.5\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR\n",
    "    x1, x2 -> AND -> NOT\n",
    "                            -> AND -> y\n",
    "    x1, x2 -> OR           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1  1]\n",
      "results (4,) [1 0 1 1]\n",
      "X [[1 2]\n",
      " [3 4]]\n",
      "X[1] [3 4]\n",
      "X[1].reshape [[3]\n",
      " [4]]\n",
      "W [[1]\n",
      " [2]]\n",
      "result [[ 1.3]\n",
      " [ 2.4]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, -1, 1, 1])\n",
    "print(arr)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "vfunc = np.vectorize(stepFunction)\n",
    "results = vfunc(arr)\n",
    "print('results', results.shape, results)\n",
    "\n",
    "\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "print('X', X)\n",
    "print('X[1]', X[1])\n",
    "print('X[1].reshape', X[1].reshape(-1, 1))\n",
    "\n",
    "W = np.array([[1], [2]])\n",
    "print('W', W)\n",
    "\n",
    "print('result', W + 0.1 * X[1].reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.78051,-0.063669,1\n",
    "0.28774,0.29139,1\n",
    "0.40714,0.17878,1\n",
    "0.2923,0.4217,1\n",
    "0.50922,0.35256,1\n",
    "0.27785,0.10802,1\n",
    "0.27527,0.33223,1\n",
    "0.43999,0.31245,1\n",
    "0.33557,0.42984,1\n",
    "0.23448,0.24986,1\n",
    "0.0084492,0.13658,1\n",
    "0.12419,0.33595,1\n",
    "0.25644,0.42624,1\n",
    "0.4591,0.40426,1\n",
    "0.44547,0.45117,1\n",
    "0.42218,0.20118,1\n",
    "0.49563,0.21445,1\n",
    "0.30848,0.24306,1\n",
    "0.39707,0.44438,1\n",
    "0.32945,0.39217,1\n",
    "0.40739,0.40271,1\n",
    "0.3106,0.50702,1\n",
    "0.49638,0.45384,1\n",
    "0.10073,0.32053,1\n",
    "0.69907,0.37307,1\n",
    "0.29767,0.69648,1\n",
    "0.15099,0.57341,1\n",
    "0.16427,0.27759,1\n",
    "0.33259,0.055964,1\n",
    "0.53741,0.28637,1\n",
    "0.19503,0.36879,1\n",
    "0.40278,0.035148,1\n",
    "0.21296,0.55169,1\n",
    "0.48447,0.56991,1\n",
    "0.25476,0.34596,1\n",
    "0.21726,0.28641,1\n",
    "0.67078,0.46538,1\n",
    "0.3815,0.4622,1\n",
    "0.53838,0.32774,1\n",
    "0.4849,0.26071,1\n",
    "0.37095,0.38809,1\n",
    "0.54527,0.63911,1\n",
    "0.32149,0.12007,1\n",
    "0.42216,0.61666,1\n",
    "0.10194,0.060408,1\n",
    "0.15254,0.2168,1\n",
    "0.45558,0.43769,1\n",
    "0.28488,0.52142,1\n",
    "0.27633,0.21264,1\n",
    "0.39748,0.31902,1\n",
    "0.5533,1,0\n",
    "0.44274,0.59205,0\n",
    "0.85176,0.6612,0\n",
    "0.60436,0.86605,0\n",
    "0.68243,0.48301,0\n",
    "1,0.76815,0\n",
    "0.72989,0.8107,0\n",
    "0.67377,0.77975,0\n",
    "0.78761,0.58177,0\n",
    "0.71442,0.7668,0\n",
    "0.49379,0.54226,0\n",
    "0.78974,0.74233,0\n",
    "0.67905,0.60921,0\n",
    "0.6642,0.72519,0\n",
    "0.79396,0.56789,0\n",
    "0.70758,0.76022,0\n",
    "0.59421,0.61857,0\n",
    "0.49364,0.56224,0\n",
    "0.77707,0.35025,0\n",
    "0.79785,0.76921,0\n",
    "0.70876,0.96764,0\n",
    "0.69176,0.60865,0\n",
    "0.66408,0.92075,0\n",
    "0.65973,0.66666,0\n",
    "0.64574,0.56845,0\n",
    "0.89639,0.7085,0\n",
    "0.85476,0.63167,0\n",
    "0.62091,0.80424,0\n",
    "0.79057,0.56108,0\n",
    "0.58935,0.71582,0\n",
    "0.56846,0.7406,0\n",
    "0.65912,0.71548,0\n",
    "0.70938,0.74041,0\n",
    "0.59154,0.62927,0\n",
    "0.45829,0.4641,0\n",
    "0.79982,0.74847,0\n",
    "0.60974,0.54757,0\n",
    "0.68127,0.86985,0\n",
    "0.76694,0.64736,0\n",
    "0.69048,0.83058,0\n",
    "0.68122,0.96541,0\n",
    "0.73229,0.64245,0\n",
    "0.76145,0.60138,0\n",
    "0.58985,0.86955,0\n",
    "0.73145,0.74516,0\n",
    "0.77029,0.7014,0\n",
    "0.73156,0.71782,0\n",
    "0.44556,0.57991,0\n",
    "0.85275,0.85987,0\n",
    "0.51912,0.62359,0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "#     return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "    logits = np.matmul(X, W) + b\n",
    "#     print('logits', logits.shape, logits)    \n",
    "        \n",
    "    vfunc = np.vectorize(stepFunction)\n",
    "    preds = vfunc(logits)\n",
    "#     print('preds', preds.shape, preds)    \n",
    "    return preds\n",
    "\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    \n",
    "#     print('X', X.shape, X)\n",
    "#     print('y', y.shape, y)\n",
    "\n",
    "#     print('W', W.shape, W)\n",
    "#     print('b', b.shape, b)\n",
    "    \n",
    "    print('W', W)\n",
    "    print('b', b)\n",
    "\n",
    "    y_pred = prediction(X, W, b)\n",
    "#     print('y_pred', y_pred.shape, y_pred)\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        if y_pred[i] != y[i]:\n",
    "            if y_pred[i] == 1:\n",
    "                print('1', y_pred[i], y[i])\n",
    "                W = W - learn_rate * X[i].reshape(-1, 1)\n",
    "                b = b - learn_rate\n",
    "            else:\n",
    "                print('0', y_pred[i], y[i])         \n",
    "                W = W + learn_rate * X[i].reshape(-1, 1)\n",
    "                b = b + learn_rate            \n",
    "    \n",
    "    print('new_W', W)\n",
    "    print('new_b', b)    \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):        \n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X, W) + b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code    \n",
    "    for i in range(y.shape[0]):\n",
    "        y_pred = prediction(X[i], W, b)\n",
    "\n",
    "        if y_pred != y[i]:\n",
    "            if y_pred == 1:\n",
    "#                 print('1', y_pred, y[i])\n",
    "                W = W - learn_rate * X[i].reshape(-1, 1)\n",
    "                b = b - learn_rate\n",
    "            else:\n",
    "#                 print('0', y_pred, y[i])         \n",
    "                W = W + learn_rate * X[i].reshape(-1, 1)\n",
    "                b = b + learn_rate            \n",
    "    \n",
    "#     print('new_W', W)\n",
    "#     print('new_b', b)    \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):        \n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.71828183   7.3890561   20.08553692]\n",
      "[ 0.09003057  0.24472847  0.66524096]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09003057,  0.24472847,  0.66524096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    exp_L = np.exp(L)\n",
    "    softmax_L = exp_L / np.sum(exp_L)\n",
    "    print(exp_L)\n",
    "    print(softmax_L)\n",
    "    return softmax_L\n",
    "    \n",
    "softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Cross-Entropy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68517901091076849"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "-np.log(0.8)-np.log(0.7)-np.log(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90386821187559785"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = [0, 1, 1]\n",
    "P = [0.1, 0.5, 0.9]\n",
    "\n",
    "Y = np.array(Y)\n",
    "P = np.array(P)\n",
    "print(Y)\n",
    "-np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.array(Y)\n",
    "    P = np.array(P)\n",
    "    \n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802183888559\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = 5\n",
    "w2 = 4\n",
    "b = -3\n",
    "\n",
    "logit = 0.4 * w1 + 0.6 * w2 + b\n",
    "\n",
    "y_hat = 1 / (1 + np.exp(-logit))\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
